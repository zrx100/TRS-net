{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant libraries and functions\n",
    "from __future__ import print_function\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage\n",
    "import numpy as np, h5py\n",
    "import os, time, sys\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Convolution2D, Input, SpatialDropout2D, UpSampling2D, MaxPooling2D, concatenate\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers import Dense, Dropout, Conv1D, Input, Conv2D, add, Conv3D, Reshape\n",
    "from keras.callbacks import History, EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from itertools import cycle\n",
    "from sklearn import metrics\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, SeparableConv2D, Conv2DTranspose\n",
    "import matplotlib.pyplot as pyplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_data = './trainbi'\n",
    "#f_data = '/gs/home/wenjiao/zrx-2DGP/Group-bi/pydata/trainbi1' # Directory with trainging data\n",
    "stacks = os.listdir(f_data)\n",
    "numS = int(len(stacks))\n",
    "\n",
    "nTG = 200 # Number of time-points\n",
    "xX = 100\n",
    "\n",
    "\n",
    "tpsfD = np.ndarray(\n",
    "        (numS, int(nTG), int(xX),  int(1)), dtype=np.float32\n",
    "        )\n",
    "t1 = np.ndarray(\n",
    "        (numS, int(xX), int(1)), dtype=np.float32\n",
    "        )\n",
    "t2 = np.ndarray(\n",
    "        (numS, int(xX), int(1)), dtype=np.float32\n",
    "        )\n",
    "tR = np.ndarray(\n",
    "        (numS, int(xX),  int(1)), dtype=np.float32\n",
    "        )\n",
    "\n",
    "i = 0;\n",
    "for d in stacks:\n",
    "    # Save values to respective mapping\n",
    "    f = h5py.File(os.path.join(f_data,d),'r') \n",
    "    tpsfD[i,:,:,0] = f.get('sigD')\n",
    "    f = h5py.File(os.path.join(f_data,d),'r') \n",
    "    t1[i,:,0] = f.get('t1')\n",
    "    f = h5py.File(os.path.join(f_data,d),'r') \n",
    "    t2[i,:,0] = f.get('t2')\n",
    "    f = h5py.File(os.path.join(f_data,d),'r') \n",
    "    tR[i,:,0] = f.get('rT')\n",
    "    i = i + 1\n",
    "\n",
    "tpsfD[np.isnan(tpsfD)]=0\n",
    "    \n",
    "tpsfD =  np.moveaxis(tpsfD, 1, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 100, 200, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure TPSF voxel shape is correct dimensionality (# samples, x, y, time-points, 1)\n",
    "tpsfD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant resblock functions (Keras API)\n",
    "def resblock_2D(num_filters, size_filter, x):\n",
    "    Fx = Conv2D(num_filters, size_filter, padding='same', activation=None)(x)\n",
    "    Fx = Activation('relu')(Fx)\n",
    "    Fx = Conv2D(num_filters, size_filter, padding='same', activation=None)(Fx)\n",
    "    output = add([Fx, x])\n",
    "    output = Activation('relu')(output)\n",
    "    return output\n",
    "\n",
    "def resblock_2D_BN(num_filters, size_filter, x):\n",
    "    Fx = Conv2D(num_filters, size_filter, padding='same', activation=None)(x)\n",
    "    Fx = BatchNormalization()(Fx) \n",
    "    Fx = Activation('relu')(Fx)\n",
    "    Fx = Conv2D(num_filters, size_filter, padding='same', activation=None)(Fx)\n",
    "    Fx = BatchNormalization()(Fx)\n",
    "    output = add([Fx, x])\n",
    "    #output = BatchNormalization()(output)\n",
    "    output = Activation('relu')(output)\n",
    "    return output\n",
    "\n",
    "def resblock_3D_BN(num_filters, size_filter, x):\n",
    "    Fx = Conv3D(num_filters, size_filter, padding='same', activation=None)(x)\n",
    "    Fx = BatchNormalization()(Fx)\n",
    "    Fx = Activation('relu')(Fx)\n",
    "    Fx = Conv3D(num_filters, size_filter, padding='same', activation=None)(Fx)\n",
    "    Fx = BatchNormalization()(Fx)\n",
    "    output = add([Fx, x])\n",
    "    output = BatchNormalization()(output)\n",
    "    output = Activation('relu')(output)\n",
    "    return output\n",
    "\n",
    "def xCeptionblock_2D_BN(num_filters, size_filter, x):\n",
    "    Fx = SeparableConv2D(num_filters, size_filter, padding='same', activation=None)(x)\n",
    "    Fx = BatchNormalization()(Fx)\n",
    "    Fx = Activation('relu')(Fx)\n",
    "    Fx = SeparableConv2D(num_filters, size_filter, padding='same', activation=None)(Fx)\n",
    "    Fx = BatchNormalization()(Fx)\n",
    "    output = add([Fx, x])\n",
    "    output = Activation('relu')(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelD = None\n",
    "xX = 100;\n",
    "\n",
    "\n",
    "t_data = Input(shape=(xX,200,1))\n",
    "tpsf = t_data\n",
    "\n",
    "# # # # # # # # 2D-Model # # # # # # # #\n",
    "\n",
    "tpsf = Conv2D(10,kernel_size=(1,10),strides=(1,5), padding='same', activation=None, data_format=\"channels_last\")(tpsf)\n",
    "tpsf = BatchNormalization()(tpsf)\n",
    "tpsf = Activation('relu')(tpsf)\n",
    "\n",
    "\n",
    "tpsf = Conv2D(256,1, padding='same', activation=None, data_format=\"channels_last\")(tpsf)\n",
    "tpsf = BatchNormalization()(tpsf)\n",
    "tpsf = Activation('relu')(tpsf)\n",
    "tpsf = Conv2D(256,1, padding='same', activation=None, data_format=\"channels_last\")(tpsf)\n",
    "tpsf = BatchNormalization()(tpsf)\n",
    "tpsf = Activation('relu')(tpsf)\n",
    "tpsf = resblock_2D_BN(256, 1,tpsf)\n",
    "tpsf = resblock_2D_BN(256, 1,tpsf)\n",
    "tpsf = Reshape((xX,10240))(tpsf)\n",
    "\n",
    "# Short-lifetime branch\n",
    "imgT1 = Dense(64, activation=None)(tpsf)\n",
    "imgT1 = BatchNormalization()(imgT1)\n",
    "imgT1 = Activation('relu')(imgT1)\n",
    "imgT1 = Dense(32, activation=None)(imgT1)\n",
    "imgT1 = BatchNormalization()(imgT1)\n",
    "imgT1 = Activation('relu')(imgT1)\n",
    "imgT1 = Dense(1, activation=None)(imgT1)\n",
    "imgT1 = Activation('relu')(imgT1)\n",
    "imgT1 = keras.layers.Dense(1, activation='relu',name='T1')(imgT1)\n",
    "# Long-lifetime branch\n",
    "imgT2 = Dense(64, activation=None)(tpsf)\n",
    "imgT2 = BatchNormalization()(imgT2)\n",
    "imgT2 = Activation('relu')(imgT2)\n",
    "imgT2 = Dense(32, activation=None)(imgT2)\n",
    "imgT2 = BatchNormalization()(imgT2)\n",
    "imgT2 = Activation('relu')(imgT2)\n",
    "imgT2 = Dense(1, activation=None)(imgT2)\n",
    "imgT2 = Activation('relu')(imgT2)\n",
    "imgT2 = keras.layers.Dense(1,activation='relu',name='T2')(imgT2)\n",
    "# Amplitude-Ratio branch\n",
    "imgTR = Dense(64, activation=None)(tpsf)\n",
    "imgTR = BatchNormalization()(imgTR)\n",
    "imgTR = Activation('relu')(imgTR)\n",
    "imgTR = Dense(32, activation=None)(imgTR)\n",
    "imgTR = BatchNormalization()(imgTR)\n",
    "imgTR = Activation('relu')(imgTR)\n",
    "imgTR = Dense(1, activation=None)(imgTR) \n",
    "imgTR = Activation('relu')(imgTR)\n",
    "imgTR = keras.layers.Dense(1,activation='relu',name='TR')(imgTR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelD = Model(inputs=[t_data], outputs=[imgT1,imgT2,imgTR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\"T1\": \"mse\",\"T2\": \"mse\",\"TR\": \"mse\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = {\"T1\": \"mae\",\"T2\": \"mae\",\"TR\": \"mae\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelD.compile(loss= losses,\n",
    "              optimizer= 'adam',\n",
    "              metrics= metric)\n",
    "# Setting patience (patience = 15 recommended)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', \n",
    "                              patience = 15, \n",
    "                              verbose = 0,\n",
    "                              mode = 'auto')\n",
    "\n",
    "fN = 'testName' # Assign some name for weights and training/validation loss curves here\n",
    "\n",
    "# Save loss curve (mse) and MAE information over all trained epochs. (monitor = '' can be changed to focus on other tau parameters)\n",
    "modelCheckPoint = ModelCheckpoint(filepath=fN+'.h5', \n",
    "                                  monitor='val_loss', \n",
    "                                  save_best_only=True, \n",
    "                                  verbose=0)\n",
    "# Train network (80/20 train/validation split, batch_size=20 recommended, nb_epoch may vary based on application)\n",
    "history = History()\n",
    "csv_logger = CSVLogger(fN+'.log')\n",
    "history = modelD.fit([tpsfD], [t1,t2,tR],\n",
    "          validation_split=0.2,\n",
    "          batch_size=20, epochs=500, verbose=1, shuffle=True, callbacks=[earlyStopping,csv_logger,modelCheckPoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# Post-training: load \"best\" trained weights (obtained through patience - lowest value of loss)\n",
    "# THIS CAN BE ANY WEIGHT FILE, AS LONG AS THE NETWORK ARCHITECTURE MATCHES THE ONE USED!\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "modelD.load_weights(fN+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload test data and use 3D-CNN for inference\n",
    "t_data = './fitbi'\n",
    "#t_data = '/gs/home/wenjiao/zrx-2DGP/Group-bi/pydata/fitbi' # directory with test data\n",
    "stacksT = os.listdir(t_data)\n",
    "numT = int(len(stacksT))\n",
    "\n",
    "nTG = 200\n",
    "xX = 100\n",
    "\n",
    "\n",
    "tpsfT = np.ndarray(\n",
    "        (numT, int(nTG), int(xX),  int(1)), dtype=np.float32\n",
    "        )\n",
    "t1T = np.ndarray(\n",
    "        (numT, int(xX), int(1)), dtype=np.float32\n",
    "        )\n",
    "t2T = np.ndarray(\n",
    "        (numT, int(xX), int(1)), dtype=np.float32\n",
    "        )\n",
    "tRT = np.ndarray(\n",
    "        (numT, int(xX),  int(1)), dtype=np.float32\n",
    "        )\n",
    "\n",
    "i = 0;\n",
    "for d in stacksT:\n",
    "    # Save values to respective mapping\n",
    "    f = h5py.File(os.path.join(t_data,d),'r') \n",
    "    tpsfT[i,:,:,0] = f.get('sigD')\n",
    "    f = h5py.File(os.path.join(t_data,d),'r') \n",
    "    t1T[i,:,0] = f.get('t1')\n",
    "    f = h5py.File(os.path.join(t_data,d),'r') \n",
    "    t2T[i,:,0] = f.get('t2')\n",
    "    f = h5py.File(os.path.join(t_data,d),'r') \n",
    "    tRT[i,:,0] = f.get('rT')\n",
    "    i = i + 1\n",
    "    \n",
    "tpsfT =  np.moveaxis(tpsfT, 1, -2)\n",
    "# tpsfT = np.moveaxis(tpsfT, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference on test data with trained model\n",
    "testV = modelD.predict(tpsfT)\n",
    "# Predicted t1 values\n",
    "\n",
    "import pickle\n",
    "\n",
    "def save_variable(v,filename):\n",
    "        f = open(filename,'wb')\n",
    "        pickle.dump(v,f)\n",
    "        f.close()\n",
    "        return filename\n",
    "\n",
    "def load_variable(filename):\n",
    "        f=open(filename,'rb')\n",
    "        r=pickle.load(f)\n",
    "        f.close()\n",
    "        return r\n",
    "\n",
    "import joblib\n",
    "joblib.dump(testV,'testV1.pkl')\n",
    "\n",
    "\n",
    "modelD.save('30_2dgp.h5')\n",
    "\n",
    "modelD.save('30_2dgp.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
